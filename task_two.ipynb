{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install pdfplumber scikit-learn PyPDF2 numpy pytesseract Pillow\n",
        "!apt-get install tesseract-ocr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m21d6rLhEhES",
        "outputId": "c1dfc9f5-c743-4d0b-a7d9-e3a3f1da8acb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: pdfminer.six==20250327 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20250327)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250327->pdfplumber) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250327->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (2.22)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install pdfplumber scikit-learn PyPDF2 numpy pytesseract Pillow\n",
        "\n",
        "# Import libraries\n",
        "from PyPDF2 import PdfReader, PdfWriter\n",
        "import os\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report\n",
        "import pdfplumber\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import io\n",
        "import numpy as np\n",
        "\n",
        "class ImprovedRealEstateDocumentProcessor:\n",
        "    def __init__(self):\n",
        "        # Enhanced keywords with more variations and synonyms\n",
        "        self.keywords = {\n",
        "            'deeds': [\n",
        "                'title to real estate', 'quitclaim deed', 'warranty deed', 'general warranty deed',\n",
        "                'special warranty deed', 'deed of trust', 'grant deed', 'bargain and sale deed',\n",
        "                'grantee', 'grantor', 'conveyance', 'transfer of ownership', 'legal description',\n",
        "                'habendum clause', 'consideration', 'acknowledgment', 'notarized'\n",
        "            ],\n",
        "            'property_cards': [\n",
        "                'property card', 'parcel id', 'land value', 'assessed value', 'market value',\n",
        "                'property assessment', 'tax assessment', 'square footage', 'lot size',\n",
        "                'building description', 'property characteristics', 'zoning', 'tax map',\n",
        "                'parcel number', 'tax id', 'real property', 'assessment roll'\n",
        "            ],\n",
        "            'tax_documents': [\n",
        "                'tax bill', 'amount due', 'tax year', 'property tax', 'real estate tax',\n",
        "                'tax assessment', 'tax collector', 'payment due', 'tax notice',\n",
        "                'delinquent tax', 'tax lien', 'installment', 'tax payment',\n",
        "                'municipal tax', 'county tax', 'school tax', 'fire tax'\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        # Enhanced patterns for document boundaries\n",
        "        self.recording_stamps = [\n",
        "            r'recorded on \\w+ \\d{1,2}, \\d{4}',\n",
        "            r'official records',\n",
        "            r'book \\d+ page \\d+',\n",
        "            r'instrument number',\n",
        "            r'recorded \\d{1,2}/\\d{1,2}/\\d{4}',\n",
        "            r'filing date',\n",
        "            r'document number',\n",
        "            r'reception number'\n",
        "        ]\n",
        "\n",
        "        # Improved ML pipeline with better parameters\n",
        "        self.model = Pipeline([\n",
        "            ('tfidf', TfidfVectorizer(\n",
        "                max_features=5000,\n",
        "                ngram_range=(1, 3),  # Include bigrams and trigrams\n",
        "                stop_words='english',\n",
        "                min_df=2,\n",
        "                max_df=0.8,\n",
        "                lowercase=True\n",
        "            )),\n",
        "            ('clf', MultinomialNB(alpha=0.1))\n",
        "        ])\n",
        "\n",
        "    def extract_text_from_pdf(self, pdf_path):\n",
        "        \"\"\"Enhanced text extraction with multiple fallback methods\"\"\"\n",
        "        text = \"\"\n",
        "\n",
        "        # Method 1: Try pdfplumber first\n",
        "        try:\n",
        "            with pdfplumber.open(pdf_path) as pdf:\n",
        "                for page in pdf.pages:\n",
        "                    if page.extract_text():\n",
        "                        text += page.extract_text() + \"\\n\"\n",
        "                    else:\n",
        "                        # Fallback to OCR for image-based pages\n",
        "                        try:\n",
        "                            img = page.to_image(resolution=300)\n",
        "                            pil_img = img.original\n",
        "                            ocr_text = pytesseract.image_to_string(pil_img)\n",
        "                            text += ocr_text + \"\\n\"\n",
        "                        except:\n",
        "                            pass\n",
        "        except Exception as e:\n",
        "            print(f\"pdfplumber failed for {pdf_path}: {e}\")\n",
        "\n",
        "        # Method 2: Fallback to PyPDF2 if pdfplumber fails\n",
        "        if not text.strip():\n",
        "            try:\n",
        "                reader = PdfReader(pdf_path)\n",
        "                for page in reader.pages:\n",
        "                    page_text = page.extract_text()\n",
        "                    if page_text:\n",
        "                        text += page_text + \"\\n\"\n",
        "            except Exception as e:\n",
        "                print(f\"PyPDF2 also failed for {pdf_path}: {e}\")\n",
        "\n",
        "        # Method 3: OCR as last resort for entire document\n",
        "        if not text.strip():\n",
        "            try:\n",
        "                with pdfplumber.open(pdf_path) as pdf:\n",
        "                    for page in pdf.pages:\n",
        "                        try:\n",
        "                            img = page.to_image(resolution=300)\n",
        "                            pil_img = img.original\n",
        "                            ocr_text = pytesseract.image_to_string(pil_img)\n",
        "                            text += ocr_text + \"\\n\"\n",
        "                        except:\n",
        "                            continue\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def extract_text_from_page_range(self, pdf_path, start_page, end_page):\n",
        "        \"\"\"Enhanced text extraction for page ranges\"\"\"\n",
        "        text = \"\"\n",
        "\n",
        "        try:\n",
        "            with pdfplumber.open(pdf_path) as pdf:\n",
        "                for i in range(start_page, end_page + 1):\n",
        "                    if i < len(pdf.pages):\n",
        "                        page = pdf.pages[i]\n",
        "                        page_text = page.extract_text()\n",
        "                        if page_text:\n",
        "                            text += page_text + \"\\n\"\n",
        "                        else:\n",
        "                            # Try OCR for this page\n",
        "                            try:\n",
        "                                img = page.to_image(resolution=300)\n",
        "                                pil_img = img.original\n",
        "                                ocr_text = pytesseract.image_to_string(pil_img)\n",
        "                                text += ocr_text + \"\\n\"\n",
        "                            except:\n",
        "                                pass\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting text from pages {start_page}-{end_page}: {e}\")\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"Clean and preprocess text for better classification\"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        # Remove extra whitespace and normalize\n",
        "        text = re.sub(r'\\s+', ' ', text.strip())\n",
        "\n",
        "        # Remove non-ASCII characters that might cause issues\n",
        "        text = ''.join(char for char in text if ord(char) < 128)\n",
        "\n",
        "        # Normalize common variations\n",
        "        text = re.sub(r'\\b(deed|deeds)\\b', 'deed', text.lower())\n",
        "        text = re.sub(r'\\b(tax|taxes)\\b', 'tax', text.lower())\n",
        "        text = re.sub(r'\\b(property|properties)\\b', 'property', text.lower())\n",
        "\n",
        "        return text\n",
        "\n",
        "    def train_model(self, training_dir):\n",
        "        \"\"\"Enhanced model training with validation\"\"\"\n",
        "        texts, labels = [], []\n",
        "\n",
        "        for doc_type in ['deeds', 'property_cards', 'tax_documents']:\n",
        "            dir_path = os.path.join(training_dir, doc_type)\n",
        "            if os.path.exists(dir_path):\n",
        "                for filename in os.listdir(dir_path):\n",
        "                    if filename.endswith('.pdf'):\n",
        "                        try:\n",
        "                            text = self.extract_text_from_pdf(os.path.join(dir_path, filename))\n",
        "                            if text:\n",
        "                                processed_text = self.preprocess_text(text)\n",
        "                                if processed_text:\n",
        "                                    texts.append(processed_text)\n",
        "                                    labels.append(doc_type)\n",
        "                                else:\n",
        "                                    print(f\"Warning: No processable text from {filename}\")\n",
        "                            else:\n",
        "                                print(f\"Warning: No text extracted from {filename}\")\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error processing {filename}: {e}\")\n",
        "            else:\n",
        "                print(f\"Warning: Training directory not found: {dir_path}\")\n",
        "\n",
        "        if texts and len(set(labels)) > 1:\n",
        "            self.model.fit(texts, labels)\n",
        "\n",
        "            # Perform cross-validation to check model performance\n",
        "            scores = cross_val_score(self.model, texts, labels, cv=min(5, len(texts)//2))\n",
        "            print(f\"Cross-validation scores: {scores}\")\n",
        "            print(f\"Average CV score: {np.mean(scores):.3f}\")\n",
        "\n",
        "            # Print classification report\n",
        "            predictions = self.model.predict(texts)\n",
        "            print(\"\\nTraining Classification Report:\")\n",
        "            print(classification_report(labels, predictions))\n",
        "\n",
        "        else:\n",
        "            print(\"Error: Insufficient training data found. Cannot train model.\")\n",
        "\n",
        "    def find_document_boundaries(self, pdf_path):\n",
        "        \"\"\"Enhanced boundary detection\"\"\"\n",
        "        boundaries = []\n",
        "\n",
        "        try:\n",
        "            with pdfplumber.open(pdf_path) as pdf:\n",
        "                full_text = \"\"\n",
        "                for i, page in enumerate(pdf.pages):\n",
        "                    page_text = page.extract_text() if page.extract_text() else \"\"\n",
        "                    # If no text, try OCR\n",
        "                    if not page_text.strip():\n",
        "                        try:\n",
        "                            img = page.to_image(resolution=300)\n",
        "                            pil_img = img.original\n",
        "                            page_text = pytesseract.image_to_string(pil_img)\n",
        "                        except:\n",
        "                            page_text = \"\"\n",
        "\n",
        "                    full_text += page_text + f\"\\n---PAGE_BREAK---{i+1}---\\n\"\n",
        "\n",
        "                # Look for recording stamps and other boundary markers\n",
        "                for pattern in self.recording_stamps:\n",
        "                    for match in re.finditer(pattern, full_text.lower()):\n",
        "                        closest_page_break = full_text[:match.end()].rfind(\"---PAGE_BREAK---\")\n",
        "                        if closest_page_break != -1:\n",
        "                            page_num_str = full_text[closest_page_break:].split(\"---PAGE_BREAK---\")[1].split(\"---\")[0]\n",
        "                            boundaries.append(int(page_num_str) - 1)\n",
        "\n",
        "                # Also look for other document separators\n",
        "                additional_patterns = [\n",
        "                    r'new document',\n",
        "                    r'document \\d+',\n",
        "                    r'page 1 of \\d+',\n",
        "                    r'return to:'\n",
        "                ]\n",
        "\n",
        "                for pattern in additional_patterns:\n",
        "                    for match in re.finditer(pattern, full_text.lower()):\n",
        "                        closest_page_break = full_text[:match.end()].rfind(\"---PAGE_BREAK---\")\n",
        "                        if closest_page_break != -1:\n",
        "                            page_num_str = full_text[closest_page_break:].split(\"---PAGE_BREAK---\")[1].split(\"---\")[0]\n",
        "                            boundaries.append(int(page_num_str) - 1)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error finding boundaries: {e}\")\n",
        "\n",
        "        if not boundaries:\n",
        "            # Fallback: assume each page is a document\n",
        "            try:\n",
        "                with pdfplumber.open(pdf_path) as pdf:\n",
        "                    return [(i, i) for i in range(len(pdf.pages))]\n",
        "            except:\n",
        "                return [(0, 0)]  # Single document fallback\n",
        "\n",
        "        boundaries = sorted(list(set(boundaries)))\n",
        "        doc_ranges = []\n",
        "        start_page = 0\n",
        "\n",
        "        for boundary_page in boundaries:\n",
        "            if boundary_page > start_page:\n",
        "                doc_ranges.append((start_page, boundary_page - 1))\n",
        "            start_page = boundary_page\n",
        "\n",
        "        # Add the last document\n",
        "        try:\n",
        "            with pdfplumber.open(pdf_path) as pdf:\n",
        "                if start_page < len(pdf.pages):\n",
        "                    doc_ranges.append((start_page, len(pdf.pages) - 1))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return doc_ranges if doc_ranges else [(0, 0)]\n",
        "\n",
        "    def classify_document(self, text):\n",
        "        \"\"\"Enhanced classification with confidence scoring\"\"\"\n",
        "        if not text:\n",
        "            return 'unknown'\n",
        "\n",
        "        processed_text = self.preprocess_text(text)\n",
        "\n",
        "        # Keyword-based classification with weighted scoring\n",
        "        scores = defaultdict(float)\n",
        "\n",
        "        for doc_type, terms in self.keywords.items():\n",
        "            for term in terms:\n",
        "                # Count occurrences and weight by term importance\n",
        "                count = processed_text.lower().count(term.lower())\n",
        "                if count > 0:\n",
        "                    # Weight longer, more specific terms higher\n",
        "                    weight = len(term.split()) * 1.5\n",
        "                    scores[doc_type] += count * weight\n",
        "\n",
        "        # Normalize scores\n",
        "        total_score = sum(scores.values())\n",
        "        if total_score > 0:\n",
        "            for doc_type in scores:\n",
        "                scores[doc_type] /= total_score\n",
        "\n",
        "        # Check if keyword classification is confident enough\n",
        "        if scores:\n",
        "            predicted_type = max(scores.items(), key=lambda x: x[1])[0]\n",
        "            confidence = scores[predicted_type]\n",
        "\n",
        "            # Lower threshold for better recall\n",
        "            if confidence > 0.3:\n",
        "                return predicted_type\n",
        "\n",
        "        # Fallback to ML model\n",
        "        try:\n",
        "            if hasattr(self.model, 'predict') and hasattr(self.model, 'predict_proba'):\n",
        "                ml_prediction = self.model.predict([processed_text])[0]\n",
        "                ml_probabilities = self.model.predict_proba([processed_text])[0]\n",
        "                max_prob = max(ml_probabilities)\n",
        "\n",
        "                # Only use ML prediction if confident enough\n",
        "                if max_prob > 0.4:\n",
        "                    return ml_prediction\n",
        "        except Exception as e:\n",
        "            print(f\"ML prediction error: {e}\")\n",
        "\n",
        "        # Final fallback: use keyword classification even with low confidence\n",
        "        if scores:\n",
        "            return max(scores.items(), key=lambda x: x[1])[0]\n",
        "\n",
        "        return 'unknown'\n",
        "\n",
        "    def process_merged_pdf(self, pdf_path, output_dir):\n",
        "        \"\"\"Enhanced PDF processing with better error handling\"\"\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        doc_ranges = self.find_document_boundaries(pdf_path)\n",
        "\n",
        "        counts = defaultdict(int)\n",
        "\n",
        "        try:\n",
        "            reader = PdfReader(pdf_path)\n",
        "            total_pages = len(reader.pages)\n",
        "            print(f\"Total pages in PDF: {total_pages}\")\n",
        "            print(f\"Found {len(doc_ranges)} document ranges\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading PDF: {e}\")\n",
        "            return\n",
        "\n",
        "        for i, (start_page, end_page) in enumerate(doc_ranges):\n",
        "            try:\n",
        "                writer = PdfWriter()\n",
        "\n",
        "                # Validate page range\n",
        "                start_page = max(0, start_page)\n",
        "                end_page = min(total_pages - 1, end_page)\n",
        "\n",
        "                if start_page > end_page:\n",
        "                    print(f\"Skipping invalid page range: {start_page}-{end_page}\")\n",
        "                    continue\n",
        "\n",
        "                # Add pages to writer\n",
        "                for page_num in range(start_page, end_page + 1):\n",
        "                    if page_num < total_pages:\n",
        "                        writer.add_page(reader.pages[page_num])\n",
        "\n",
        "                # Extract and classify text\n",
        "                doc_text = self.extract_text_from_page_range(pdf_path, start_page, end_page)\n",
        "\n",
        "                if doc_text:\n",
        "                    doc_type = self.classify_document(doc_text)\n",
        "                    counts[doc_type] += 1\n",
        "                    filename = f\"{doc_type.replace('_', '-')}_{counts[doc_type]}.pdf\"\n",
        "                    output_filepath = os.path.join(output_dir, filename)\n",
        "\n",
        "                    # Save the document\n",
        "                    with open(output_filepath, \"wb\") as f:\n",
        "                        writer.write(f)\n",
        "\n",
        "                    print(f\"Saved {filename} (pages {start_page + 1}-{end_page + 1})\")\n",
        "\n",
        "                    # Print a sample of the extracted text for debugging\n",
        "                    sample_text = doc_text[:200].replace('\\n', ' ')\n",
        "                    print(f\"  Sample text: {sample_text}...\")\n",
        "\n",
        "                else:\n",
        "                    print(f\"Warning: No text extracted for document in range {start_page}-{end_page}\")\n",
        "\n",
        "                    # Still save the document but mark as unknown\n",
        "                    counts['unknown'] += 1\n",
        "                    filename = f\"unknown_{counts['unknown']}.pdf\"\n",
        "                    output_filepath = os.path.join(output_dir, filename)\n",
        "\n",
        "                    with open(output_filepath, \"wb\") as f:\n",
        "                        writer.write(f)\n",
        "\n",
        "                    print(f\"Saved {filename} (pages {start_page + 1}-{end_page + 1}) - no text extracted\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing document {i}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Print summary\n",
        "        print(f\"\\nProcessing Summary:\")\n",
        "        for doc_type, count in counts.items():\n",
        "            print(f\"  {doc_type}: {count} documents\")\n",
        "\n",
        "#  PATHS\n",
        "train_path = \"/content/drive/MyDrive/training_data\"\n",
        "merged_pdf_path = \"/content/drive/MyDrive/merged_document/Sample_Full_Search_5.pdf\"\n",
        "output_dir = \"/content/drive/MyDrive/real_estate_project/split_output\"\n",
        "\n",
        "# Run the improved processor\n",
        "processor = ImprovedRealEstateDocumentProcessor()\n",
        "processor.train_model(train_path)\n",
        "processor.process_merged_pdf(merged_pdf_path, output_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QM-fkj4Eovo",
        "outputId": "a88d266a-07f7-458d-ee58-4929215a4d1e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: pdfminer.six==20250327 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20250327)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250327->pdfplumber) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250327->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250327->pdfplumber) (2.22)\n",
            "Cross-validation scores: [1. 1. 1. 1. 1.]\n",
            "Average CV score: 1.000\n",
            "\n",
            "Training Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "         deeds       1.00      1.00      1.00        50\n",
            "property_cards       1.00      1.00      1.00        50\n",
            " tax_documents       1.00      1.00      1.00        50\n",
            "\n",
            "      accuracy                           1.00       150\n",
            "     macro avg       1.00      1.00      1.00       150\n",
            "  weighted avg       1.00      1.00      1.00       150\n",
            "\n",
            "Total pages in PDF: 70\n",
            "Found 8 document ranges\n",
            "Saved tax-documents_1.pdf (pages 1-7)\n",
            "  Sample text: Date: November 30, 2021 Client: Cobb Dill & Hammett Attn: Accounting Department RE: Invoice for Abstracting Services Please see the invoice details below for title abstracting services. Please make ch...\n",
            "Saved deeds_1.pdf (pages 8-8)\n",
            "  Sample text: Administrator Login WELCOME REAL PROPERTY RECORD SEARCH REAL PROPERTY BILL SEARCH PERSONAL PROPERTY SEARCH MOTOR VEHICLE SEARCH CHECKOUT CONTACT US ( $0.00 CHECK OUT ! RETURN \" SALES # TAX INFO $ ADDI...\n",
            "Saved property-cards_1.pdf (pages 9-32)\n",
            "  Sample text: Sorting order Date-Time Date-Time-Maker Date-Maker-Time Date-Recipient-Time Record Record Maker Firm Name Recipient Firm Name Inst Book- Orig Orig Date Time Page Book Page 10:00:39 BANK OF SOUTH 0956-...\n",
            "Saved tax-documents_2.pdf (pages 33-34)\n",
            "  Sample text: \" Can I help? ! Home Lien ID Individual Business Search for an Individual Find a state tax lien using an individual's first and last name below. The name in this registry is based on the information p...\n",
            "Saved property-cards_2.pdf (pages 35-39)\n",
            "  Sample text: NC 28216- Lien Records Filter 5610 USA Lien Taxpayer Spouse Address Issued BalancePayoff Date Date Partial 8ID76295CNaUmNeNIN or DBA 2053 Statewi5d5e6.55Payoff 0Fi3le-dSepS-2at0is2f1i e1Rd9e:4le7a:s4e...\n",
            "Saved deeds_2.pdf (pages 40-55)\n",
            "  Sample text: Search Criteria Last Name / Firm Name: cunningham* First Name / Firm Name: lawrence* Record Date: Greater Than 2021-01-27 Record Date: Less Than 2021-11-29 DM-Name_MIK Search Results Click on a Column...\n",
            "Saved deeds_3.pdf (pages 56-59)\n",
            "  Sample text: Exhibit A  ALL that certain lot, piece or parcel of land, together with all buildings and improvements thereon, if any, situate, lying and being in the City of Charleston, County of Charleston, State ...\n",
            "Saved deeds_4.pdf (pages 60-70)\n",
            "  Sample text: RECORDER'S PAGE  NOTE: This page MUST remain with the original document  Filed By: COBB DILL & HAMMETT LLC (COURIER) 222 W COLEMAN BLVD #2     MT PLEASANT SC 29464-3588  MAKER:  CUNNINGHAM LAWRENCE H ...\n",
            "\n",
            "Processing Summary:\n",
            "  tax_documents: 2 documents\n",
            "  deeds: 4 documents\n",
            "  property_cards: 2 documents\n"
          ]
        }
      ]
    }
  ]
}